{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Continual Learning for Saudi Dialect Offensive Language Detection\n",
        "## Under Temporal Linguistic Drift\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Afefa-Asiri/Continual-Learning-for-Saudi-Dialect-Offensive-Language-Detection-under-Temporal-Linguistic-Drift/blob/main/CL_Experiments.ipynb)\n",
        "\n",
        "This notebook implements the complete experiment pipeline for evaluating continual learning methods on Saudi dialect offensive language detection.\n",
        "\n",
        "### Experiment Structure\n",
        "- **Phase 1**: LoRA Ablation Study (4 ranks \u00d7 5 seeds = 20 runs)\n",
        "- **Phase 2**: Main CL Experiments (8 methods \u00d7 5 seeds = 40 runs)\n",
        "\n",
        "### Methods Evaluated\n",
        "| Method | Description |\n",
        "|--------|-------------|\n",
        "| Original | Baseline (no adaptation) |\n",
        "| Na\u00efve FT | Standard fine-tuning |\n",
        "| ER | Experience Replay |\n",
        "| EWC | Elastic Weight Consolidation |\n",
        "| LoRA | Low-Rank Adaptation |\n",
        "| LoRA+ER | LoRA with Experience Replay |\n",
        "| LoRA+EWC | LoRA with EWC |\n",
        "| LoRA+ER+EWC | LoRA with both ER and EWC |\n",
        "| Full+ER+EWC | Full fine-tuning with ER and EWC |\n",
        "\n",
        "### Key Implementation Details\n",
        "1. **EWC Formula**: `loss + (\u03bb/2) * penalty` (Kirkpatrick et al., 2017)\n",
        "2. **Validation**: NEW-ONLY (replay samples used only in training, not validation)\n",
        "3. **LoRA**: Applied to Q, K, V attention projection matrices\n",
        "4. **Statistical Robustness**: 5 random seeds with mean \u00b1 std reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"\u2713 Google Drive already mounted!\")\n",
        "else:\n",
        "    print(\"\ud83d\udcc2 Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"\u2713 Google Drive mounted!\")\n",
        "\n",
        "# Disable wandb\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['WANDB_MODE'] = 'disabled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets peft scikit-learn seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "import warnings\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "\u26a0\ufe0f **UPDATE THE PATHS BELOW** to point to your data files in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#==============================================================================\n",
        "# DATA PATHS - UPDATE THESE TO YOUR PATHS\n",
        "#==============================================================================\n",
        "BASE_PATH = '/content/drive/MyDrive/YOUR_FOLDER/data'  # <-- UPDATE THIS\n",
        "\n",
        "# Model checkpoint\n",
        "MODEL_PATH = f'{BASE_PATH}/SOD_AraBERT_model'\n",
        "\n",
        "# Training data\n",
        "ORIGINAL_DS = f'{BASE_PATH}/SDOffensive_Paper2.csv'      # Original SOD (2019-2022)\n",
        "NEW_DS = f'{BASE_PATH}/Paper2_DS_Complete.csv'           # New data (2024-2025)\n",
        "\n",
        "# Test sets\n",
        "HISTORICAL_TEST = f'{BASE_PATH}/processed500UnseenDS_Paper2.csv'  # Historical test\n",
        "CONTEMPORARY_TEST = f'{BASE_PATH}/Balanced500_Paper2.csv'         # Contemporary test\n",
        "MIXED_2080_TEST = f'{BASE_PATH}/TestDS2080_Paper2.csv'            # Mixed 80-20\n",
        "MIXED_4060_TEST = f'{BASE_PATH}/TestDS4060_Paper2.csv'            # Mixed 40-60\n",
        "\n",
        "# Output directories\n",
        "RESULTS_DIR = '/content/drive/MyDrive/YOUR_FOLDER/results'  # <-- UPDATE THIS\n",
        "FIGURES_DIR = os.path.join(RESULTS_DIR, 'figures')\n",
        "TABLES_DIR = os.path.join(RESULTS_DIR, 'tables')\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(TABLES_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved to: {RESULTS_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#==============================================================================\n",
        "# HYPERPARAMETERS\n",
        "#==============================================================================\n",
        "SEEDS = [42, 101, 123, 456, 789]  # 5 random seeds for statistical robustness\n",
        "REPLAY_SAMPLES = 750              # Class-balanced replay buffer size\n",
        "EWC_LAMBDA = 1000                 # EWC regularization strength\n",
        "EWC_SAMPLES = 1000                # Samples for Fisher computation\n",
        "MAX_LENGTH = 128                  # Maximum sequence length\n",
        "BATCH_SIZE = 32                   # Batch size\n",
        "LEARNING_RATE = 2e-5              # Learning rate (standard for BERT)\n",
        "EPOCHS = 5                        # Training epochs\n",
        "CLASS_WEIGHT_0 = 1.0              # Weight for non-offensive class\n",
        "CLASS_WEIGHT_1 = 2.0              # Weight for offensive class\n",
        "\n",
        "# Model size (AraBERT ~ 135M parameters)\n",
        "TOTAL_MODEL_PARAMS = 135_000_000\n",
        "\n",
        "print(\"Hyperparameters configured:\")\n",
        "print(f\"  Seeds: {SEEDS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  EWC \u03bb: {EWC_LAMBDA}\")\n",
        "print(f\"  Replay buffer: {REPLAY_SAMPLES} samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA Ablation Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#==============================================================================\n",
        "# LoRA ABLATION VARIANTS (4 ranks, Q/K/V only)\n",
        "#==============================================================================\n",
        "LORA_ABLATION_VARIANTS = [\n",
        "    {\"name\": \"r=8\",  \"r\": 8,  \"alpha\": 16,  \"target_modules\": [\"query\", \"key\", \"value\"], \"dropout\": 0.1, \"bias\": \"none\"},\n",
        "    {\"name\": \"r=16\", \"r\": 16, \"alpha\": 32,  \"target_modules\": [\"query\", \"key\", \"value\"], \"dropout\": 0.1, \"bias\": \"none\"},\n",
        "    {\"name\": \"r=32\", \"r\": 32, \"alpha\": 64,  \"target_modules\": [\"query\", \"key\", \"value\"], \"dropout\": 0.1, \"bias\": \"none\"},\n",
        "    {\"name\": \"r=64\", \"r\": 64, \"alpha\": 128, \"target_modules\": [\"query\", \"key\", \"value\"], \"dropout\": 0.1, \"bias\": \"none\"},\n",
        "]\n",
        "\n",
        "# Optimal config (will be determined by ablation)\n",
        "OPTIMAL_LORA_CONFIG = None\n",
        "\n",
        "print(\"LoRA variants to test:\")\n",
        "for v in LORA_ABLATION_VARIANTS:\n",
        "    print(f\"  {v['name']}: r={v['r']}, \u03b1={v['alpha']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Settings (Colorblind-friendly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#==============================================================================\n",
        "# VISUALIZATION SETTINGS\n",
        "#==============================================================================\n",
        "COLORS = {\n",
        "    'blue': '#0072B2', 'orange': '#E69F00', 'green': '#009E73',\n",
        "    'red': '#D55E00', 'purple': '#CC79A7', 'yellow': '#F0E442',\n",
        "    'cyan': '#56B4E9', 'gray': '#999999', 'brown': '#8B4513',\n",
        "}\n",
        "\n",
        "METHOD_COLORS = {\n",
        "    'original': COLORS['gray'], 'naive_ft': COLORS['orange'],\n",
        "    'er': COLORS['blue'], 'ewc': COLORS['purple'],\n",
        "    'lora': COLORS['green'], 'lora+er': COLORS['yellow'],\n",
        "    'lora+ewc': COLORS['cyan'], 'lora+er+ewc': COLORS['red'],\n",
        "    'full+er+ewc': COLORS['brown'],\n",
        "}\n",
        "\n",
        "METHOD_NAMES = {\n",
        "    'original': 'Original', 'naive_ft': 'Na\u00efve FT',\n",
        "    'er': 'ER', 'ewc': 'EWC', 'lora': 'LoRA',\n",
        "    'lora+er': 'LoRA+ER', 'lora+ewc': 'LoRA+EWC',\n",
        "    'lora+er+ewc': 'LoRA+ER+EWC', 'full+er+ewc': 'Full+ER+EWC',\n",
        "}\n",
        "\n",
        "METHODS_ORDER = ['original', 'naive_ft', 'er', 'ewc', 'lora', \n",
        "                 'lora+er', 'full+er+ewc', 'lora+ewc', 'lora+er+ewc']\n",
        "\n",
        "TEST_SET_NAMES = {\n",
        "    'historical': 'Historical', 'contemporary': 'Contemporary',\n",
        "    'mixed_2080': 'Mixed 80-20', 'mixed_4060': 'Mixed 40-60',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_dataset_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load dataset from CSV file.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    if 'labels' in df.columns and 'label' not in df.columns:\n",
        "        df = df.rename(columns={'labels': 'label'})\n",
        "    return df[['text', 'label']].copy()\n",
        "\n",
        "\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    \"\"\"Tokenize text examples.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_test_dataset(df: pd.DataFrame) -> Dataset:\n",
        "    \"\"\"Prepare test dataset for evaluation.\"\"\"\n",
        "    return Dataset.from_pandas(df[['text', 'label']].rename(columns={'label': 'labels'}))\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
        "        'recall_macro': recall_score(labels, preds, average='macro', zero_division=0),\n",
        "        'f1_macro': f1_score(labels, preds, average='macro', zero_division=0),\n",
        "        'f1_class_0': f1_score(labels, preds, pos_label=0, average='binary', zero_division=0),\n",
        "        'f1_class_1': f1_score(labels, preds, pos_label=1, average='binary', zero_division=0),\n",
        "    }\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def fmt_mean_std(values, decimals=4):\n",
        "    \"\"\"Format mean \u00b1 std string.\"\"\"\n",
        "    values = [v for v in values if v is not None and not np.isnan(v)]\n",
        "    if not values:\n",
        "        return \"N/A\"\n",
        "    m, s = np.mean(values), np.std(values)\n",
        "    if len(values) == 1:\n",
        "        return f\"{m:.{decimals}f}\"\n",
        "    return f\"{m:.{decimals}f} \u00b1 {s:.{decimals}f}\"\n",
        "\n",
        "\n",
        "print(\"\u2713 Utility functions defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. EWC Implementation\n",
        "\n",
        "Elastic Weight Consolidation (Kirkpatrick et al., 2017) prevents catastrophic forgetting by:\n",
        "1. Computing a diagonal Fisher Information Matrix to identify important parameters\n",
        "2. Adding a penalty term that discourages changes to important weights\n",
        "\n",
        "**Formula**: `L_total = L_task + (\u03bb/2) * \u03a3 F_i * (\u03b8_i - \u03b8*_i)\u00b2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class EWC:\n",
        "    \"\"\"\n",
        "    Elastic Weight Consolidation (Kirkpatrick et al., 2017)\n",
        "    \n",
        "    Computes diagonal Fisher Information Matrix to identify important\n",
        "    parameters for previous tasks and penalizes changes to them.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, dataloader, device):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.params = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self.fisher = self._compute_fisher(dataloader)\n",
        "\n",
        "    def _compute_fisher(self, dataloader):\n",
        "        \"\"\"Compute diagonal Fisher Information Matrix.\"\"\"\n",
        "        fisher = {n: torch.zeros_like(p, device=self.device) \n",
        "                  for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "        self.model.eval()\n",
        "        num_samples = 0\n",
        "        \n",
        "        for batch in dataloader:\n",
        "            self.model.zero_grad()\n",
        "            inputs = {k: v.to(self.device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(self.device)\n",
        "\n",
        "            outputs = self.model(**inputs)\n",
        "            loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if p.requires_grad and p.grad is not None:\n",
        "                    fisher[n] += p.grad.detach() ** 2\n",
        "            num_samples += labels.size(0)\n",
        "\n",
        "        for n in fisher:\n",
        "            fisher[n] /= max(1, num_samples)\n",
        "\n",
        "        return fisher\n",
        "\n",
        "    def penalty(self, model):\n",
        "        \"\"\"Compute EWC penalty term.\"\"\"\n",
        "        loss = 0.0\n",
        "        for n, p in model.named_parameters():\n",
        "            if n in self.fisher:\n",
        "                loss += (self.fisher[n] * (p - self.params[n]) ** 2).sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "print(\"\u2713 EWC class defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Trainer with EWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class EWCTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom Trainer with EWC regularization and class weights.\n",
        "    \n",
        "    Uses the correct EWC formula: loss + (\u03bb/2) * penalty\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, *args, ewc=None, ewc_lambda=0.0, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ewc = ewc\n",
        "        self.ewc_lambda = ewc_lambda\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
        "        else:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        # EWC regularization: loss + (\u03bb/2) * penalty\n",
        "        if self.ewc is not None and self.ewc_lambda > 0:\n",
        "            loss = loss + (self.ewc_lambda / 2) * self.ewc.penalty(model)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "print(\"\u2713 EWCTrainer class defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Experience Replay\n",
        "\n",
        "Creates a class-balanced replay buffer from the original training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_replay_buffer(original_df: pd.DataFrame, n: int, seed: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create class-balanced replay buffer from original training data.\n",
        "    \n",
        "    Args:\n",
        "        original_df: Original training dataset\n",
        "        n: Number of samples for replay buffer\n",
        "        seed: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with balanced replay samples\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    df0 = original_df[original_df['label'] == 0]\n",
        "    df1 = original_df[original_df['label'] == 1]\n",
        "\n",
        "    n0, n1 = n // 2, n - n // 2\n",
        "\n",
        "    replay0 = df0.sample(n=min(n0, len(df0)), replace=(len(df0) < n0), random_state=seed)\n",
        "    replay1 = df1.sample(n=min(n1, len(df1)), replace=(len(df1) < n1), random_state=seed)\n",
        "\n",
        "    return pd.concat([replay0, replay1]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"\u2713 Experience Replay function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Main Training Function\n",
        "\n",
        "\u26a0\ufe0f **IMPORTANT**: Validation uses NEW data only (replay samples used only in training).\n",
        "This ensures validation measures true adaptation to the new distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_method(",
        "    method_name: str,",
        "    seed: int,",
        "    original_df: pd.DataFrame,",
        "    new_df: pd.DataFrame,",
        "    test_datasets: Dict,",
        "    tokenizer,",
        "    use_lora: bool = False,",
        "    use_er: bool = False,",
        "    use_ewc: bool = False,",
        "    lora_config: dict = None,",
        ") -> Tuple[Dict, float, Dict, int, Dict]:",
        "    \"\"\"",
        "    Train a continual learning method and evaluate on all test sets.",
        "    ",
        "    IMPORTANT: Validation uses NEW data only (replay only in training).",
        "    This ensures validation measures true adaptation to new distribution.",
        "    \"\"\"",
        "",
        "    print(f\"\\n{'='*60}\")",
        "    print(f\"Training: {method_name} | Seed: {seed}\")",
        "    print(f\"LoRA: {use_lora} | ER: {use_er} | EWC: {use_ewc}\")",
        "    print(f\"{'='*60}\")",
        "",
        "    set_seed(seed)",
        "    start_time = time.time()",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
        "",
        "    # Step 1: Split NEW data into train/val (NEW-ONLY validation)",
        "    new_train_df, new_val_df = train_test_split(",
        "        new_df, test_size=0.2, random_state=seed, stratify=new_df['label']",
        "    )",
        "    print(f\"  New data: {len(new_train_df)} train / {len(new_val_df)} val\")",
        "    ",
        "    # Step 2: Add replay to training ONLY (not validation)",
        "    if use_er:",
        "        replay_df = create_replay_buffer(original_df, REPLAY_SAMPLES, seed)",
        "        train_df = pd.concat([new_train_df, replay_df], ignore_index=True)",
        "        train_df = train_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)",
        "        print(f\"  ER: {len(new_train_df)} new + {len(replay_df)} replay = {len(train_df)} total\")",
        "    else:",
        "        train_df = new_train_df",
        "        print(f\"  Training: {len(train_df)} samples\")",
        "    ",
        "    # Step 3: Validation is NEW-ONLY",
        "    val_df = new_val_df",
        "    print(f\"  Validation: {len(val_df)} samples (NEW-ONLY)\")",
        "",
        "    # Tokenize datasets",
        "    train_ds = Dataset.from_dict({'text': train_df['text'].tolist(), 'labels': train_df['label'].tolist()})",
        "    val_ds = Dataset.from_dict({'text': val_df['text'].tolist(), 'labels': val_df['label'].tolist()})",
        "    train_ds = train_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)",
        "    val_ds = val_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)",
        "",
        "    # Load model",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)",
        "    model = model.to(device)",
        "",
        "    # Apply LoRA if specified",
        "    if use_lora and lora_config:",
        "        peft_config = LoraConfig(",
        "            r=lora_config[\"r\"],",
        "            lora_alpha=lora_config[\"alpha\"],",
        "            target_modules=lora_config[\"target_modules\"],",
        "            lora_dropout=lora_config.get(\"dropout\", 0.1),",
        "            bias=lora_config.get(\"bias\", \"none\"),",
        "            task_type=TaskType.SEQ_CLS",
        "        )",
        "        model = get_peft_model(model, peft_config)",
        "        print(f\"  LoRA: r={lora_config['r']}, \u03b1={lora_config['alpha']}\")",
        "        model.print_trainable_parameters()",
        "",
        "    # Count parameters",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)",
        "    total_params = sum(p.numel() for p in model.parameters())",
        "    print(f\"  Params: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")",
        "",
        "    # Compute EWC Fisher Information if needed",
        "    ewc = None",
        "    if use_ewc:",
        "        print(\"  Computing Fisher Information...\")",
        "        ewc_df = original_df.sample(n=min(EWC_SAMPLES, len(original_df)), random_state=seed)",
        "        ewc_ds = Dataset.from_pandas(ewc_df[['text', 'label']].rename(columns={'label': 'labels'}))",
        "        ewc_ds = ewc_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)",
        "        ewc_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])",
        "        ewc_loader = DataLoader(ewc_ds, batch_size=BATCH_SIZE, shuffle=False)",
        "        ewc = EWC(model, ewc_loader, device)",
        "        print(\"  \u2713 EWC Fisher computed\")",
        "",
        "    # Training",
        "    class_weights = torch.tensor([CLASS_WEIGHT_0, CLASS_WEIGHT_1], dtype=torch.float32)",
        "    ",
        "    training_args = TrainingArguments(",
        "        output_dir=f'./temp_{method_name}_{seed}',",
        "        num_train_epochs=EPOCHS,",
        "        per_device_train_batch_size=BATCH_SIZE,",
        "        per_device_eval_batch_size=BATCH_SIZE,",
        "        learning_rate=LEARNING_RATE,",
        "        warmup_steps=100,",
        "        weight_decay=0.01,",
        "        eval_strategy='epoch',",
        "        save_strategy='no',",
        "        report_to=\"none\",",
        "        logging_steps=50,",
        "        fp16=torch.cuda.is_available(),",
        "        seed=seed,",
        "        remove_unused_columns=False,",
        "    )",
        "",
        "    trainer = EWCTrainer(",
        "        model=model,",
        "        args=training_args,",
        "        train_dataset=train_ds,",
        "        eval_dataset=val_ds,",
        "        compute_metrics=compute_metrics,",
        "        ewc=ewc,",
        "        ewc_lambda=EWC_LAMBDA if use_ewc else 0.0,",
        "        class_weights=class_weights",
        "    )",
        "",
        "    trainer.train()",
        "",
        "    # Extract convergence metrics",
        "    log_history = trainer.state.log_history or []",
        "    train_losses = [x.get('loss') for x in log_history if 'loss' in x]",
        "    eval_losses = [x.get('eval_loss') for x in log_history if 'eval_loss' in x]",
        "    ",
        "    convergence = {",
        "        'final_train_loss': float(train_losses[-1]) if train_losses else float('nan'),",
        "        'best_val_loss': float(min(eval_losses)) if eval_losses else float('nan'),",
        "        'final_val_loss': float(eval_losses[-1]) if eval_losses else float('nan'),",
        "    }",
        "",
        "    training_time = time.time() - start_time",
        "",
        "    # Evaluate on all test sets",
        "    results, predictions = {}, {}",
        "    for test_name, test_ds in test_datasets.items():",
        "        results[test_name] = trainer.evaluate(test_ds)",
        "        pred_output = trainer.predict(test_ds)",
        "        predictions[f'{test_name}_preds'] = np.argmax(pred_output.predictions, axis=1)",
        "        predictions[f'{test_name}_labels'] = pred_output.label_ids",
        "",
        "    print(f\"\u2713 Completed in {training_time:.1f}s\")",
        "",
        "    del model, trainer",
        "    clear_memory()",
        "",
        "    return results, training_time, convergence, trainable_params, predictions",
        "",
        "",
        "print(\"\u2713 Training function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Original Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_original_model(test_datasets: Dict, tokenizer) -> Tuple[Dict, Dict]:",
        "    \"\"\"",
        "    Evaluate original model (deterministic baseline).",
        "    Single evaluation - no std needed since no training involved.",
        "    \"\"\"",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"Evaluating: ORIGINAL MODEL (Baseline)\")",
        "    print(\"=\"*60)",
        "    ",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
        "    model = model.to(device)",
        "    ",
        "    trainer = Trainer(",
        "        model=model,",
        "        args=TrainingArguments(",
        "            output_dir='./temp', ",
        "            per_device_eval_batch_size=BATCH_SIZE, ",
        "            report_to=\"none\", ",
        "            remove_unused_columns=False",
        "        ),",
        "        compute_metrics=compute_metrics",
        "    )",
        "",
        "    results, predictions = {}, {}",
        "    for test_name, test_ds in test_datasets.items():",
        "        results[test_name] = trainer.evaluate(test_ds)",
        "        pred_output = trainer.predict(test_ds)",
        "        predictions[f'{test_name}_preds'] = np.argmax(pred_output.predictions, axis=1)",
        "        predictions[f'{test_name}_labels'] = pred_output.label_ids",
        "        ",
        "        print(f\"  {test_name}: F1={results[test_name]['eval_f1_macro']:.4f}, \"",
        "              f\"Acc={results[test_name]['eval_accuracy']:.4f}\")",
        "    ",
        "    del model, trainer",
        "    clear_memory()",
        "    ",
        "    print(\"\u2713 Original model evaluation complete\")",
        "    return results, predictions",
        "",
        "",
        "print(\"\u2713 Original model evaluation function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Phase 1: LoRA Ablation Study\n",
        "\n",
        "Tests 4 ranks (8, 16, 32, 64) \u00d7 5 seeds = 20 runs.\n",
        "Selection criterion: Balanced score = (Historical F1 + Contemporary F1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_lora_ablation(original_df, new_df, test_datasets, tokenizer):",
        "    \"\"\"",
        "    Run LoRA ablation study to determine optimal rank.",
        "    Tests 4 ranks \u00d7 5 seeds = 20 runs.",
        "    \"\"\"",
        "    ",
        "    print(\"\\n\" + \"=\"*80)",
        "    print(\"PHASE 1: LoRA ABLATION STUDY\")",
        "    print(\"Testing 4 ranks \u00d7 5 seeds = 20 runs\")",
        "    print(\"=\"*80)",
        "",
        "    ablation_results = {}",
        "    ablation_params = {}",
        "",
        "    for variant in LORA_ABLATION_VARIANTS:",
        "        variant_name = variant[\"name\"]",
        "        print(f\"\\n{'#'*60}\")",
        "        print(f\"LoRA Rank: {variant_name} (r={variant['r']}, \u03b1={variant['alpha']})\")",
        "        print(f\"{'#'*60}\")",
        "",
        "        ablation_results[variant_name] = []",
        "        ",
        "        for seed in SEEDS:",
        "            results, train_time, conv, trainable, preds = train_method(",
        "                method_name=f\"LoRA_{variant_name}\",",
        "                seed=seed,",
        "                original_df=original_df,",
        "                new_df=new_df,",
        "                test_datasets=test_datasets,",
        "                tokenizer=tokenizer,",
        "                use_lora=True,",
        "                use_er=False,",
        "                use_ewc=False,",
        "                lora_config=variant,",
        "            )",
        "            ablation_results[variant_name].append(results)",
        "            ablation_params[variant_name] = trainable",
        "",
        "    # Determine optimal rank",
        "    print(\"\\n\" + \"=\"*60)",
        "    print(\"ABLATION RESULTS SUMMARY\")",
        "    print(\"=\"*60)",
        "    print(f\"{'Rank':<10} | {'Historical':<12} | {'Contemporary':<12} | {'Balanced':<12}\")",
        "    print(\"-\" * 55)",
        "",
        "    best_variant = None",
        "    best_score = -1",
        "",
        "    for variant in LORA_ABLATION_VARIANTS:",
        "        name = variant[\"name\"]",
        "        results_list = ablation_results[name]",
        "        ",
        "        hist_f1 = np.mean([r['historical']['eval_f1_macro'] for r in results_list])",
        "        cont_f1 = np.mean([r['contemporary']['eval_f1_macro'] for r in results_list])",
        "        balanced_score = (hist_f1 + cont_f1) / 2",
        "        ",
        "        print(f\"{name:<10} | {hist_f1:.4f}       | {cont_f1:.4f}       | {balanced_score:.4f}\")",
        "        ",
        "        if balanced_score > best_score:",
        "            best_score = balanced_score",
        "            best_variant = variant",
        "",
        "    print(f\"\\n\u2713 OPTIMAL RANK: {best_variant['name']} (Balanced: {best_score:.4f})\")",
        "    ",
        "    return ablation_results, ablation_params, best_variant",
        "",
        "",
        "print(\"\u2713 LoRA ablation function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Phase 2: Main Experiments\n",
        "\n",
        "Runs 8 continual learning methods \u00d7 5 seeds = 40 runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_main_experiments(original_df, new_df, test_datasets, tokenizer, optimal_lora_config,",
        "                         original_results, original_predictions):",
        "    \"\"\"",
        "    Run main continual learning experiments using optimal LoRA config.",
        "    8 methods \u00d7 5 seeds = 40 runs.",
        "    \"\"\"",
        "    ",
        "    print(\"\\n\" + \"=\"*80)",
        "    print(\"PHASE 2: MAIN CONTINUAL LEARNING EXPERIMENTS\")",
        "    print(f\"Using optimal LoRA: {optimal_lora_config['name']}\")",
        "    print(\"=\"*80)",
        "",
        "    methods_config = [",
        "        ('naive_ft', False, False, False),",
        "        ('er', False, True, False),",
        "        ('ewc', False, False, True),",
        "        ('lora', True, False, False),",
        "        ('lora+er', True, True, False),",
        "        ('lora+ewc', True, False, True),",
        "        ('lora+er+ewc', True, True, True),",
        "        ('full+er+ewc', False, True, True),",
        "    ]",
        "",
        "    # Initialize with original results",
        "    all_results = {'original': [original_results]}",
        "    training_times = {'original': [0.0]}",
        "    convergence_metrics = {'original': [{}]}",
        "    params_dict = {'original': 0}",
        "    all_predictions = {'original': original_predictions}",
        "",
        "    for method_name, use_lora, use_er, use_ewc in methods_config:",
        "        print(f\"\\n{'#'*60}\")",
        "        print(f\"Method: {method_name.upper()}\")",
        "        print(f\"{'#'*60}\")",
        "",
        "        all_results[method_name] = []",
        "        training_times[method_name] = []",
        "        convergence_metrics[method_name] = []",
        "",
        "        for seed in SEEDS:",
        "            results, train_time, conv, trainable, preds = train_method(",
        "                method_name=method_name,",
        "                seed=seed,",
        "                original_df=original_df,",
        "                new_df=new_df,",
        "                test_datasets=test_datasets,",
        "                tokenizer=tokenizer,",
        "                use_lora=use_lora,",
        "                use_er=use_er,",
        "                use_ewc=use_ewc,",
        "                lora_config=optimal_lora_config if use_lora else None,",
        "            )",
        "",
        "            all_results[method_name].append(results)",
        "            training_times[method_name].append(train_time)",
        "            convergence_metrics[method_name].append(conv)",
        "            params_dict[method_name] = trainable",
        "            ",
        "            # Store predictions from last seed for confusion matrix",
        "            if seed == SEEDS[-1]:",
        "                all_predictions[method_name] = preds",
        "",
        "    return all_results, training_times, convergence_metrics, params_dict, all_predictions",
        "",
        "",
        "print(\"\u2713 Main experiments function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Table Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_all_tables(all_results, training_times, convergence_metrics, params_dict,",
        "                        ablation_results, ablation_params, optimal_lora_config):",
        "    \"\"\"Generate all result tables.\"\"\"",
        "",
        "    # Original model F1 for KR/AG calculation",
        "    orig_hist = all_results['original'][0]['historical']['eval_f1_macro']",
        "    orig_cont = all_results['original'][0]['contemporary']['eval_f1_macro']",
        "",
        "    # TABLE 3: Overall Performance",
        "    print(\"\\n\" + \"=\"*100)",
        "    print(\"TABLE 3: Overall Performance Across Evaluation Scenarios\")",
        "    print(\"=\"*100)",
        "",
        "    table3_rows = []",
        "    for method in METHODS_ORDER:",
        "        if method not in all_results:",
        "            continue",
        "        results_list = all_results[method]",
        "        row = {'Method': METHOD_NAMES.get(method, method)}",
        "        is_original = (method == 'original')",
        "        ",
        "        for test_key, test_name in TEST_SET_NAMES.items():",
        "            f1_vals = [r[test_key]['eval_f1_macro'] for r in results_list]",
        "            acc_vals = [r[test_key]['eval_accuracy'] for r in results_list]",
        "            ",
        "            if is_original:",
        "                row[f'{test_name} F1'] = f\"{f1_vals[0]:.4f}\"",
        "                row[f'{test_name} Acc'] = f\"{acc_vals[0]:.4f}\"",
        "            else:",
        "                row[f'{test_name} F1'] = fmt_mean_std(f1_vals)",
        "                row[f'{test_name} Acc'] = fmt_mean_std(acc_vals)",
        "        table3_rows.append(row)",
        "",
        "    table3 = pd.DataFrame(table3_rows)",
        "    print(table3.to_string(index=False))",
        "    table3.to_csv(os.path.join(TABLES_DIR, 'Table3_OverallPerformance.csv'), index=False)",
        "",
        "    # TABLE 4: KR and AG Analysis",
        "    print(\"\\n\" + \"=\"*100)",
        "    print(\"TABLE 4: Knowledge Retention (KR) and Adaptation Gain (AG)\")",
        "    print(\"=\"*100)",
        "",
        "    table4_rows = []",
        "    for method in METHODS_ORDER:",
        "        if method == 'original' or method not in all_results:",
        "            continue",
        "        results_list = all_results[method]",
        "        ",
        "        kr_vals = [r['historical']['eval_f1_macro'] - orig_hist for r in results_list]",
        "        ag_vals = [r['contemporary']['eval_f1_macro'] - orig_cont for r in results_list]",
        "        hist_vals = [r['historical']['eval_f1_macro'] for r in results_list]",
        "        cont_vals = [r['contemporary']['eval_f1_macro'] for r in results_list]",
        "        ",
        "        table4_rows.append({",
        "            'Method': METHOD_NAMES.get(method, method),",
        "            'KR': fmt_mean_std(kr_vals),",
        "            'AG': fmt_mean_std(ag_vals),",
        "            'Historical F1': fmt_mean_std(hist_vals),",
        "            'Contemporary F1': fmt_mean_std(cont_vals),",
        "        })",
        "",
        "    table4 = pd.DataFrame(table4_rows)",
        "    print(table4.to_string(index=False))",
        "    table4.to_csv(os.path.join(TABLES_DIR, 'Table4_KR_AG.csv'), index=False)",
        "",
        "    # TABLE 5: Training Convergence",
        "    print(\"\\n\" + \"=\"*100)",
        "    print(\"TABLE 5: Training Convergence Metrics\")",
        "    print(\"=\"*100)",
        "",
        "    table5_rows = []",
        "    for method in METHODS_ORDER:",
        "        if method not in convergence_metrics:",
        "            continue",
        "        if method == 'original':",
        "            table5_rows.append({'Method': 'Original', 'Final Train Loss': '\u2014', ",
        "                               'Best Val Loss': '\u2014', 'Final Val Loss': '\u2014',",
        "                               'Trainable Params': '\u2014', 'Time (s)': '\u2014'})",
        "            continue",
        "",
        "        conv_list = convergence_metrics[method]",
        "        final_train = [c.get('final_train_loss') for c in conv_list if c]",
        "        best_val = [c.get('best_val_loss') for c in conv_list if c]",
        "        final_val = [c.get('final_val_loss') for c in conv_list if c]",
        "        times = training_times.get(method, [])",
        "        params = params_dict.get(method, 0)",
        "",
        "        table5_rows.append({",
        "            'Method': METHOD_NAMES.get(method, method),",
        "            'Final Train Loss': fmt_mean_std([v for v in final_train if v and not np.isnan(v)]),",
        "            'Best Val Loss': fmt_mean_std([v for v in best_val if v and not np.isnan(v)]),",
        "            'Final Val Loss': fmt_mean_std([v for v in final_val if v and not np.isnan(v)]),",
        "            'Trainable Params': f\"{params:,}\" if params else '\u2014',",
        "            'Time (s)': fmt_mean_std(times, 1) if times else '\u2014'",
        "        })",
        "",
        "    table5 = pd.DataFrame(table5_rows)",
        "    print(table5.to_string(index=False))",
        "    table5.to_csv(os.path.join(TABLES_DIR, 'Table5_Convergence.csv'), index=False)",
        "",
        "    # TABLE B1: LoRA Ablation Results",
        "    print(\"\\n\" + \"=\"*100)",
        "    print(\"TABLE B1: LoRA Rank Ablation Results\")",
        "    print(\"=\"*100)",
        "",
        "    tableB1_rows = []",
        "    for variant in LORA_ABLATION_VARIANTS:",
        "        name = variant[\"name\"]",
        "        if name not in ablation_results:",
        "            continue",
        "        ",
        "        params = ablation_params.get(name, 0)",
        "        results_list = ablation_results[name]",
        "        hist_vals = [r['historical']['eval_f1_macro'] for r in results_list]",
        "        cont_vals = [r['contemporary']['eval_f1_macro'] for r in results_list]",
        "        kr_vals = [h - orig_hist for h in hist_vals]",
        "        ",
        "        is_optimal = \"\u2713\" if name == optimal_lora_config[\"name\"] else \"\"",
        "        ",
        "        tableB1_rows.append({",
        "            'Rank': name, 'Parameters': f\"{params:,}\",",
        "            'Historical F1': fmt_mean_std(hist_vals),",
        "            'Contemporary F1': fmt_mean_std(cont_vals),",
        "            'KR': fmt_mean_std(kr_vals), 'Optimal': is_optimal",
        "        })",
        "",
        "    tableB1 = pd.DataFrame(tableB1_rows)",
        "    print(tableB1.to_string(index=False))",
        "    tableB1.to_csv(os.path.join(TABLES_DIR, 'TableB1_LoRA_Ablation.csv'), index=False)",
        "",
        "    print(f\"\\n\u2713 All tables saved to: {TABLES_DIR}\")",
        "    ",
        "    return orig_hist, orig_cont",
        "",
        "",
        "print(\"\u2713 Table generation function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Figure Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_figure_1(all_results):",
        "    \"\"\"Figure 1: Performance bar charts.\"\"\"",
        "    print(\"\\n\ud83d\udcca Generating Figure 1: Performance Comparison...\")",
        "    ",
        "    test_sets = ['historical', 'contemporary', 'mixed_2080', 'mixed_4060']",
        "    methods = METHODS_ORDER",
        "    test_set_colors = ['#D55E00', '#0072B2', '#CC79A7', '#009E73']",
        "    test_set_labels = ['Historical', 'Contemporary', 'Mixed 80-20', 'Mixed 40-60']",
        "    ",
        "    fig, ax = plt.subplots(figsize=(14, 6))",
        "    x = np.arange(len(methods))",
        "    width = 0.2",
        "    ",
        "    for i, (test_set, color, label) in enumerate(zip(test_sets, test_set_colors, test_set_labels)):",
        "        means, stds = [], []",
        "        for method in methods:",
        "            if method not in all_results:",
        "                means.append(0); stds.append(0); continue",
        "            vals = [r[test_set]['eval_f1_macro'] for r in all_results[method]]",
        "            means.append(np.mean(vals))",
        "            stds.append(np.std(vals) if len(vals) > 1 else 0.0)",
        "        ",
        "        ax.bar(x + i*width, means, width, yerr=stds, label=label, ",
        "               color=color, capsize=3, edgecolor='black', linewidth=0.5, alpha=0.85)",
        "    ",
        "    ax.set_ylabel('F1-macro', fontsize=12, fontweight='bold')",
        "    ax.set_xticks(x + 1.5*width)",
        "    ax.set_xticklabels([METHOD_NAMES.get(m, m) for m in methods], fontsize=10)",
        "    ax.set_ylim(0.5, 1.05)",
        "    ax.legend(loc='upper right', fontsize=10)",
        "    ax.yaxis.grid(True, linestyle='--', alpha=0.7)",
        "    ax.set_axisbelow(True)",
        "    plt.title('Figure 1. Performance Comparison (mean \u00b1 std, 5 seeds)', fontsize=13, fontweight='bold')",
        "    plt.tight_layout()",
        "    ",
        "    fig_path = os.path.join(FIGURES_DIR, 'Figure_1_Performance.png')",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')",
        "    plt.show()",
        "    print(f\"\u2713 Saved: {fig_path}\")",
        "",
        "",
        "def generate_figure_2(all_results, orig_hist, orig_cont):",
        "    \"\"\"Figure 2: Knowledge Retention vs Adaptation Gain scatter plot.\"\"\"",
        "    print(\"\\n\ud83d\udcca Generating Figure 2: KR vs AG Trade-off...\")",
        "    ",
        "    fig, ax = plt.subplots(figsize=(10, 8))",
        "    markers = {'naive_ft': 'o', 'er': 's', 'ewc': '^', 'lora': 'D', ",
        "               'lora+er': 'v', 'lora+ewc': '<', 'lora+er+ewc': '>', 'full+er+ewc': 'p'}",
        "    ",
        "    for method in METHODS_ORDER:",
        "        if method == 'original' or method not in all_results:",
        "            continue",
        "        results_list = all_results[method]",
        "        kr_vals = [r['historical']['eval_f1_macro'] - orig_hist for r in results_list]",
        "        ag_vals = [r['contemporary']['eval_f1_macro'] - orig_cont for r in results_list]",
        "        kr_mean, kr_std = np.mean(kr_vals), np.std(kr_vals)",
        "        ag_mean, ag_std = np.mean(ag_vals), np.std(ag_vals)",
        "        ",
        "        ax.errorbar(ag_mean, kr_mean, xerr=ag_std, yerr=kr_std,",
        "                   marker=markers.get(method, 'o'), markersize=12,",
        "                   color=METHOD_COLORS.get(method, 'gray'),",
        "                   label=METHOD_NAMES.get(method, method),",
        "                   capsize=4, linewidth=2, markeredgecolor='black', markeredgewidth=0.5)",
        "    ",
        "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)",
        "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)",
        "    ax.text(0.15, 0.02, 'Ideal\\n(Adapt + Retain)', fontsize=10, ha='center', color='green', fontweight='bold')",
        "    ax.text(0.15, -0.06, 'Adapt but Forget', fontsize=10, ha='center', color='orange', fontweight='bold')",
        "    ax.text(-0.05, 0.02, 'Retain but\\nNot Adapt', fontsize=10, ha='center', color='blue', fontweight='bold')",
        "    ",
        "    ax.set_xlabel('Adaptation Gain (AG)', fontsize=12, fontweight='bold')",
        "    ax.set_ylabel('Knowledge Retention (KR)', fontsize=12, fontweight='bold')",
        "    ax.set_title('Figure 2. Knowledge Retention vs Adaptation Gain Trade-off', fontsize=13, fontweight='bold')",
        "    ax.legend(loc='lower right', fontsize=9)",
        "    ax.grid(True, linestyle='--', alpha=0.5)",
        "    plt.tight_layout()",
        "    ",
        "    fig_path = os.path.join(FIGURES_DIR, 'Figure_2_KR_AG_Tradeoff.png')",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')",
        "    plt.show()",
        "    print(f\"\u2713 Saved: {fig_path}\")",
        "",
        "",
        "def generate_figure_3(all_results, params_dict):",
        "    \"\"\"Figure 3: Parameter Efficiency (two panels).\"\"\"",
        "    print(\"\\n\ud83d\udcca Generating Figure 3: Parameter Efficiency...\")",
        "    ",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))",
        "    methods_to_plot = ['naive_ft', 'er', 'ewc', 'lora', 'lora+er', 'lora+ewc', 'lora+er+ewc', 'full+er+ewc']",
        "    ",
        "    for ax, (test_set, title) in zip(axes, [",
        "        ('historical', 'Historical F1 (Knowledge Retention)'),",
        "        ('contemporary', 'Contemporary F1 (Adaptation)')",
        "    ]):",
        "        for method in methods_to_plot:",
        "            if method not in all_results:",
        "                continue",
        "            vals = [r[test_set]['eval_f1_macro'] for r in all_results[method]]",
        "            mean_f1, std_f1 = np.mean(vals), np.std(vals) if len(vals) > 1 else 0.0",
        "            params = params_dict.get(method, TOTAL_MODEL_PARAMS)",
        "            pct = (params / TOTAL_MODEL_PARAMS) * 100",
        "            ",
        "            ax.errorbar(pct, mean_f1, yerr=std_f1, marker='o', markersize=10,",
        "                       color=METHOD_COLORS.get(method, 'gray'),",
        "                       label=METHOD_NAMES.get(method, method), capsize=4, linewidth=2)",
        "        ",
        "        ax.set_xlabel('Trainable Parameters (%)', fontsize=11, fontweight='bold')",
        "        ax.set_ylabel('F1-macro', fontsize=11, fontweight='bold')",
        "        ax.set_title(title, fontsize=12, fontweight='bold')",
        "        ax.set_xscale('log')",
        "        ax.set_ylim(0.70, 1.0)",
        "        ax.grid(True, linestyle='--', alpha=0.5)",
        "        ax.legend(loc='lower right', fontsize=8)",
        "    ",
        "    plt.suptitle('Figure 3. Parameter Efficiency', fontsize=13, fontweight='bold')",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])",
        "    ",
        "    fig_path = os.path.join(FIGURES_DIR, 'Figure_3_ParamEfficiency.png')",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')",
        "    plt.show()",
        "    print(f\"\u2713 Saved: {fig_path}\")",
        "",
        "",
        "def generate_figure_4(training_times):",
        "    \"\"\"Figure 4: Training Time Comparison.\"\"\"",
        "    print(\"\\n\ud83d\udcca Generating Figure 4: Training Time...\")",
        "    ",
        "    methods = [m for m in METHODS_ORDER if m != 'original' and m in training_times]",
        "    fig, ax = plt.subplots(figsize=(10, 6))",
        "    ",
        "    names, means, stds, colors_list = [], [], [], []",
        "    for method in methods:",
        "        times = training_times[method]",
        "        names.append(METHOD_NAMES.get(method, method))",
        "        means.append(np.mean(times))",
        "        stds.append(np.std(times) if len(times) > 1 else 0.0)",
        "        colors_list.append(METHOD_COLORS.get(method, COLORS['gray']))",
        "    ",
        "    x_pos = np.arange(len(names))",
        "    ax.bar(x_pos, means, yerr=stds, capsize=4, color=colors_list,",
        "           edgecolor='black', linewidth=0.5, alpha=0.85)",
        "    ax.set_ylabel('Training Time (seconds)', fontsize=12, fontweight='bold')",
        "    ax.set_xticks(x_pos)",
        "    ax.set_xticklabels(names, rotation=0, ha='center', fontsize=10)",
        "    ax.yaxis.grid(True, linestyle='--', alpha=0.7)",
        "    ax.set_axisbelow(True)",
        "    plt.title('Figure 4. Training Time Comparison (mean \u00b1 std, 5 seeds)', fontsize=13, fontweight='bold')",
        "    plt.tight_layout()",
        "    ",
        "    fig_path = os.path.join(FIGURES_DIR, 'Figure_4_TrainingTime.png')",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')",
        "    plt.show()",
        "    print(f\"\u2713 Saved: {fig_path}\")",
        "",
        "",
        "def generate_figure_B1(ablation_results, optimal_lora_config):",
        "    \"\"\"Figure B1: LoRA Rank Ablation Study.\"\"\"",
        "    print(\"\\n\ud83d\udcca Generating Figure B1: LoRA Ablation...\")",
        "    ",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))",
        "    ranks = [8, 16, 32, 64]",
        "    x_pos = np.arange(len(ranks))",
        "    ",
        "    for ax, (metric, title) in zip(axes, [",
        "        ('historical', 'Historical F1 (Knowledge Retention)'),",
        "        ('contemporary', 'Contemporary F1 (Adaptation)')",
        "    ]):",
        "        means, stds = [], []",
        "        for variant in LORA_ABLATION_VARIANTS:",
        "            name = variant[\"name\"]",
        "            vals = [r[metric]['eval_f1_macro'] for r in ablation_results[name]]",
        "            means.append(np.mean(vals))",
        "            stds.append(np.std(vals))",
        "        ",
        "        bars = ax.bar(x_pos, means, yerr=stds, capsize=4, ",
        "                     color=COLORS['blue'], edgecolor='black', linewidth=0.5, alpha=0.85)",
        "        ",
        "        opt_idx = [v[\"name\"] for v in LORA_ABLATION_VARIANTS].index(optimal_lora_config[\"name\"])",
        "        bars[opt_idx].set_edgecolor('red')",
        "        bars[opt_idx].set_linewidth(3)",
        "        ",
        "        ax.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')",
        "        ax.set_ylabel('F1-macro', fontsize=12, fontweight='bold')",
        "        ax.set_title(title, fontsize=13, fontweight='bold')",
        "        ax.set_xticks(x_pos)",
        "        ax.set_xticklabels([str(r) for r in ranks], fontsize=11)",
        "        ax.yaxis.grid(True, linestyle='--', alpha=0.7)",
        "        ax.set_axisbelow(True)",
        "        ax.set_ylim(0.60, 1.0)",
        "    ",
        "    plt.suptitle('Figure B1. LoRA Rank Ablation (optimal in red)', fontsize=14, fontweight='bold')",
        "    plt.tight_layout(rect=[0, 0, 1, 0.93])",
        "    ",
        "    fig_path = os.path.join(FIGURES_DIR, 'Figure_B1_LoRA_Ablation.png')",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')",
        "    plt.show()",
        "    print(f\"\u2713 Saved: {fig_path}\")",
        "",
        "",
        "print(\"\u2713 Figure generation functions defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Load Data and Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load datasets\n",
        "print(\"\ud83d\udce5 Loading datasets...\")\n",
        "original_df = load_dataset_csv(ORIGINAL_DS)\n",
        "new_df = load_dataset_csv(NEW_DS)\n",
        "historical_df = load_dataset_csv(HISTORICAL_TEST)\n",
        "contemporary_df = load_dataset_csv(CONTEMPORARY_TEST)\n",
        "mixed_2080_df = load_dataset_csv(MIXED_2080_TEST)\n",
        "mixed_4060_df = load_dataset_csv(MIXED_4060_TEST)\n",
        "\n",
        "print(f\"\u2713 Original SOD: {len(original_df)} samples\")\n",
        "print(f\"\u2713 New (NEW_DS): {len(new_df)} samples\")\n",
        "print(f\"\u2713 Historical test: {len(historical_df)}\")\n",
        "print(f\"\u2713 Contemporary test: {len(contemporary_df)}\")\n",
        "print(f\"\u2713 Mixed 80-20: {len(mixed_2080_df)}\")\n",
        "print(f\"\u2713 Mixed 40-60: {len(mixed_4060_df)}\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"\\n\ud83d\udce5 Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "print(\"\u2713 Tokenizer loaded\")\n",
        "\n",
        "# Prepare test datasets\n",
        "print(\"\\n\ud83d\udce5 Preparing test datasets...\")\n",
        "test_datasets = {}\n",
        "for name, df in [('historical', historical_df), ('contemporary', contemporary_df),\n",
        "                 ('mixed_2080', mixed_2080_df), ('mixed_4060', mixed_4060_df)]:\n",
        "    ds = prepare_test_dataset(df)\n",
        "    test_datasets[name] = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
        "print(\"\u2713 Test datasets ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Original Model (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate Original Model\n",
        "original_results, original_predictions = evaluate_original_model(test_datasets, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 1: LoRA Ablation Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run LoRA Ablation\n",
        "ablation_results, ablation_params, optimal_lora_config = run_lora_ablation(\n",
        "    original_df, new_df, test_datasets, tokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2: Main Continual Learning Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run Main Experiments\n",
        "all_results, training_times, convergence_metrics, params_dict, all_predictions = run_main_experiments(\n",
        "    original_df, new_df, test_datasets, tokenizer, optimal_lora_config,\n",
        "    original_results, original_predictions\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Generate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate Tables\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING TABLES\")\n",
        "print(\"=\"*80)\n",
        "orig_hist, orig_cont = generate_all_tables(\n",
        "    all_results, training_times, convergence_metrics, params_dict,\n",
        "    ablation_results, ablation_params, optimal_lora_config\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate Figures\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING FIGURES\")\n",
        "print(\"=\"*80)\n",
        "generate_figure_1(all_results)\n",
        "generate_figure_2(all_results, orig_hist, orig_cont)\n",
        "generate_figure_3(all_results, params_dict)\n",
        "generate_figure_4(training_times)\n",
        "generate_figure_B1(ablation_results, optimal_lora_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\u2705 ALL EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n\ud83d\udcc1 Results saved to: {RESULTS_DIR}\")\n",
        "print(f\"\\n\ud83d\udccb Tables generated:\")\n",
        "print(\"   - Table 3: Overall Performance\")\n",
        "print(\"   - Table 4: Knowledge Retention and Adaptation Gain\")\n",
        "print(\"   - Table 5: Training Convergence\")\n",
        "print(\"   - Table B1: LoRA Rank Ablation\")\n",
        "print(f\"\\n\ud83d\udcc8 Figures generated:\")\n",
        "print(\"   - Figure 1: Performance Comparison\")\n",
        "print(\"   - Figure 2: KR vs AG Trade-off\")\n",
        "print(\"   - Figure 3: Parameter Efficiency\")\n",
        "print(\"   - Figure 4: Training Time\")\n",
        "print(\"   - Figure B1: LoRA Ablation\")\n",
        "print(f\"\\n\ud83c\udfc6 Optimal LoRA rank: {optimal_lora_config['name']}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}